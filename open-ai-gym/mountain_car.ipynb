{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Mountain_car_problem\n",
    "# environment: https://github.com/openai/gym/wiki/MountainCar-v0\n",
    "# 3 actions: 0:push_left, 1:no_push, 2:push_right\n",
    "# 2 observations: 0:position ; 1:volecity\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializations\n",
    "number_states = 40 # number_of_states\n",
    "max_iteration = 5000 # max_iteration(episodes)\n",
    "initial_learning_rate = 1.0 # initial learning rate\n",
    "min_learning_rate = 0.005   # minimum learning rate\n",
    "max_step = 10000 # max_step\n",
    "\n",
    "# parameters for q learning\n",
    "epsilon = 0.05\n",
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_to_state(environment, observation):\n",
    "    # map an observation to state\n",
    "    environment_low = environment.observation_space.low\n",
    "    environment_high = environment.observation_space.high\n",
    "    environment_dx = (environment_high - environment_low) / number_states\n",
    "\n",
    "    # observation[0]:position ;  observation[1]: velocity\n",
    "    p = int((observation[0] - environment_low[0])/environment_dx[0])\n",
    "    v = int((observation[1] - environment_low[1])/environment_dx[1])\n",
    "    # p:position, v:volecity\n",
    "    return p, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_simulation(environment, policy=None, render=False):\n",
    "    observation= environment.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    for _ in range(max_step):\n",
    "        if policy is None:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            p,v = observation_to_state(environment, observation)\n",
    "            action = policy[p][v]\n",
    "        if render:\n",
    "            environment.render()\n",
    "        # proceed environment for each step\n",
    "        # get observation, reward and done after each step\n",
    "        observation, reward, done, _ = environment.step(action)\n",
    "        total_reward += gamma ** step_count * reward\n",
    "        step_count += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 -- Total Reward : -210.\n",
      "Iteration No: 51 -- Total Reward : -212.\n",
      "Iteration No: 101 -- Total Reward : -208.\n",
      "Iteration No: 151 -- Total Reward : -207.\n",
      "Iteration No: 201 -- Total Reward : -206.\n",
      "Iteration No: 251 -- Total Reward : -213.\n",
      "Iteration No: 301 -- Total Reward : -1.\n",
      "Iteration No: 351 -- Total Reward : -210.\n",
      "Iteration No: 401 -- Total Reward : -213.\n",
      "Iteration No: 451 -- Total Reward : -210.\n",
      "Iteration No: 501 -- Total Reward : -211.\n",
      "Iteration No: 551 -- Total Reward : -213.\n",
      "Iteration No: 601 -- Total Reward : -208.\n",
      "Iteration No: 651 -- Total Reward : -209.\n",
      "Iteration No: 701 -- Total Reward : -215.\n",
      "Iteration No: 751 -- Total Reward : -211.\n",
      "Iteration No: 801 -- Total Reward : -210.\n",
      "Iteration No: 851 -- Total Reward : -205.\n",
      "Iteration No: 901 -- Total Reward : -1.\n",
      "Iteration No: 951 -- Total Reward : -207.\n",
      "Iteration No: 1001 -- Total Reward : -218.\n",
      "Iteration No: 1051 -- Total Reward : -212.\n",
      "Iteration No: 1101 -- Total Reward : -210.\n",
      "Iteration No: 1151 -- Total Reward : -218.\n",
      "Iteration No: 1201 -- Total Reward : -215.\n",
      "Iteration No: 1251 -- Total Reward : -219.\n",
      "Iteration No: 1301 -- Total Reward : -212.\n",
      "Iteration No: 1351 -- Total Reward : -212.\n",
      "Iteration No: 1401 -- Total Reward : -211.\n",
      "Iteration No: 1451 -- Total Reward : -215.\n",
      "Iteration No: 1501 -- Total Reward : -214.\n",
      "Iteration No: 1551 -- Total Reward : -218.\n",
      "Iteration No: 1601 -- Total Reward : -209.\n",
      "Iteration No: 1651 -- Total Reward : -218.\n",
      "Iteration No: 1701 -- Total Reward : -207.\n",
      "Iteration No: 1751 -- Total Reward : -210.\n",
      "Iteration No: 1801 -- Total Reward : -209.\n",
      "Iteration No: 1851 -- Total Reward : -207.\n",
      "Iteration No: 1901 -- Total Reward : -213.\n",
      "Iteration No: 1951 -- Total Reward : -211.\n",
      "Iteration No: 2001 -- Total Reward : -217.\n",
      "Iteration No: 2051 -- Total Reward : -209.\n",
      "Iteration No: 2101 -- Total Reward : -211.\n",
      "Iteration No: 2151 -- Total Reward : -216.\n",
      "Iteration No: 2201 -- Total Reward : -215.\n",
      "Iteration No: 2251 -- Total Reward : -214.\n",
      "Iteration No: 2301 -- Total Reward : -1.\n",
      "Iteration No: 2351 -- Total Reward : -209.\n",
      "Iteration No: 2401 -- Total Reward : -214.\n",
      "Iteration No: 2451 -- Total Reward : -213.\n",
      "Iteration No: 2501 -- Total Reward : -209.\n",
      "Iteration No: 2551 -- Total Reward : -1.\n",
      "Iteration No: 2601 -- Total Reward : -211.\n",
      "Iteration No: 2651 -- Total Reward : -209.\n",
      "Iteration No: 2701 -- Total Reward : -210.\n",
      "Iteration No: 2751 -- Total Reward : -215.\n",
      "Iteration No: 2801 -- Total Reward : -214.\n",
      "Iteration No: 2851 -- Total Reward : -216.\n",
      "Iteration No: 2901 -- Total Reward : -212.\n",
      "Iteration No: 2951 -- Total Reward : -210.\n",
      "Iteration No: 3001 -- Total Reward : -209.\n",
      "Iteration No: 3051 -- Total Reward : -208.\n",
      "Iteration No: 3101 -- Total Reward : -217.\n",
      "Iteration No: 3151 -- Total Reward : -212.\n",
      "Iteration No: 3201 -- Total Reward : -216.\n",
      "Iteration No: 3251 -- Total Reward : -209.\n",
      "Iteration No: 3301 -- Total Reward : -213.\n",
      "Iteration No: 3351 -- Total Reward : -210.\n",
      "Iteration No: 3401 -- Total Reward : -219.\n",
      "Iteration No: 3451 -- Total Reward : -214.\n",
      "Iteration No: 3501 -- Total Reward : -208.\n",
      "Iteration No: 3551 -- Total Reward : -207.\n",
      "Iteration No: 3601 -- Total Reward : -206.\n",
      "Iteration No: 3651 -- Total Reward : -209.\n",
      "Iteration No: 3701 -- Total Reward : -210.\n",
      "Iteration No: 3751 -- Total Reward : -208.\n",
      "Iteration No: 3801 -- Total Reward : -1.\n",
      "Iteration No: 3851 -- Total Reward : -208.\n",
      "Iteration No: 3901 -- Total Reward : -213.\n",
      "Iteration No: 3951 -- Total Reward : -208.\n",
      "Iteration No: 4001 -- Total Reward : -205.\n",
      "Iteration No: 4051 -- Total Reward : -211.\n",
      "Iteration No: 4101 -- Total Reward : -207.\n",
      "Iteration No: 4151 -- Total Reward : -210.\n",
      "Iteration No: 4201 -- Total Reward : -208.\n",
      "Iteration No: 4251 -- Total Reward : -210.\n",
      "Iteration No: 4301 -- Total Reward : -207.\n",
      "Iteration No: 4351 -- Total Reward : -211.\n",
      "Iteration No: 4401 -- Total Reward : -214.\n",
      "Iteration No: 4451 -- Total Reward : -211.\n",
      "Iteration No: 4501 -- Total Reward : -207.\n",
      "Iteration No: 4551 -- Total Reward : -212.\n",
      "Iteration No: 4601 -- Total Reward : -208.\n",
      "Iteration No: 4651 -- Total Reward : -213.\n",
      "Iteration No: 4701 -- Total Reward : -213.\n",
      "Iteration No: 4751 -- Total Reward : -214.\n",
      "Iteration No: 4801 -- Total Reward : -208.\n",
      "Iteration No: 4851 -- Total Reward : -214.\n",
      "Iteration No: 4901 -- Total Reward : -208.\n",
      "Iteration No: 4951 -- Total Reward : -206.\n",
      "Mean score :  -200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-200.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # use gym environment: MountainCar-v0\n",
    "    # https://github.com/openai/gym/wiki/MountainCar-v0\n",
    "    environment_name = 'MountainCar-v0'\n",
    "    environment = gym.make(environment_name)\n",
    "    environment.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # create qTable with zeros\n",
    "    # 3 actions: 0:push_left, 1:no_push, 2:push_right\n",
    "    q_table = np.zeros((number_states, number_states, 3))\n",
    "\n",
    "    # training for maximum iteration episodes\n",
    "    for i in range(max_iteration):\n",
    "        observation = environment.reset()\n",
    "        total_reward = 0\n",
    "        # eta: learning rate is decreased at each step\n",
    "        eta = max(min_learning_rate, initial_learning_rate * (0.85 ** (i//100)))\n",
    "        # each episode is max_step long\n",
    "        for j in range(max_step):\n",
    "            p, v = observation_to_state(environment, observation)\n",
    "            # select an action\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                # get random action\n",
    "                action = np.random.choice(environment.action_space.n)\n",
    "            else:\n",
    "                logits = q_table[p][v]\n",
    "                action_p = np.argmax(q_table[p][v])\n",
    "                # calculate the exponential of all elements in the input array.\n",
    "                logits_exp = np.exp(logits)\n",
    "                # calculate the probabilities\n",
    "                probabilities = logits_exp / np.sum(logits_exp)\n",
    "                # get random action\n",
    "                action = np.random.choice(environment.action_space.n, p=probabilities)\n",
    "                # get observation, reward and done after each step\n",
    "                observation, reward, done, _ = environment.step(action_p)\n",
    "\n",
    "            total_reward += reward\n",
    "            # update q table\n",
    "            # p:position, v:volecity\n",
    "            p_, v_ = observation_to_state(environment, observation)\n",
    "            # gamma: discount factor\n",
    "            # Bellmann eq: Q(s,a)=reward + gamma* max(Q(s_,a_))  ::: Q_target = reward+gamma*max(Qs_prime)\n",
    "            q_table[p][v][action] = q_table[p][v][action] + eta * (reward + gamma *  np.max(q_table[p_][v_]) - q_table[p][v][action])\n",
    "            if done:\n",
    "                break\n",
    "        if i % 50 == 0:\n",
    "            print('Iteration No: %d -- Total Reward : %d.' %(i+1, total_reward))\n",
    "\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [episode_simulation(environment, solution_policy, False) for _ in range(100)]\n",
    "    print(\"Mean score : \", np.mean(solution_policy_scores))\n",
    "# run with render=True for visualization\n",
    "episode_simulation(environment, solution_policy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
